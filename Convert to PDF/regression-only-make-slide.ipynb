{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Method - 3\n",
    "\n",
    "The huge imbalance in the dataset and very few examples of data makes a Classifier to generalize properly. Instead of framing the problem as a Mutli-Class Classiification problem it will be better to try to frame it as a Regression Problem. The predicted values then can be mapped back to class labels.\n",
    "\n",
    "The precense of lot of categorical features in the dataset that have a notable significance with the score makes Tree based models an ideal choice. DecisionTreeRegressor is the simples tree based Regressor model that we can implement. In order to make the solution more robust and capture more insights an ensemble method can help. For that a GradientBoostingRegressor can be trained. Finally a Linear Regression model that performed well on the LB can be used.\n",
    "\n",
    "The predictions from these three models will be weighted averaged and then that prediction can be mapped to class labels or scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f461b31ae99a449de62ca3623f6be8da9883d79d",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocessing and Feature Engineering\n",
    "\n",
    "In this section the problems found in the train and test set in EDA will be fixed in this section.\n",
    "\n",
    "Also, the facility features can be target encoded. It was observed in the EDA that there is a chance for the earthling who stayed in Dec-Feb period to give higher scores. I will call that period winter. Including that information can help the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "925381e4512a4808e27d2d1b8c97bc3c82ecaef2",
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# fixing the 'hotel_stars' feature in the dataset\n",
    "\n",
    "train['hotel_stars'] = train['hotel_stars'].map(lambda z : z.replace(',', '.'))\n",
    "test['hotel_stars'] = test['hotel_stars'].map(lambda z : z.replace(',', '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3672033f199cf2ce8d91489e4728ef8d17b39280",
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# fix mars_membership_years feature in the test data\n",
    "\n",
    "test['mars_membership_years'] = test['mars_membership_years'].map(lambda y : 0 if y < 0 else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65120756a8bddfcceb0aa118641f0c5ef2f0344c",
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We have seen in the EDA that features earthling_country and hotel_name have no much significance in the dataset\n",
    "# Let's drop that from the dataset\n",
    "\n",
    "del train['earthling_country']\n",
    "del test['earthling_country']\n",
    "\n",
    "del train['hotel_name']\n",
    "del test['hotel_name']\n",
    "\n",
    "# WE have seen that in the period 'Dec-Feb' there is a higher chance to get higher score\n",
    "# Let's create a feature stayed_in_winter to indicate whether the earthling stayed in winter or not\n",
    "\n",
    "train['stayed_in_winter'] = train.period_of_stay.map(lambda x : 1 if x == 'Dec-Feb' else 0)\n",
    "test['stayed_in_winter'] = test.period_of_stay.map(lambda x : 1 if x == 'Dec-Feb' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc1af64ecf0d8fb981c9257727c2429c06361a7c",
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's label encode the categorical columns for further processing\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == object:\n",
    "        enc = LabelEncoder()\n",
    "        enc.fit(train[col])\n",
    "        train[col] = enc.transform(train[col])\n",
    "        test[col] = enc.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e8302e4546cfdda600af537f85a71d696a38515",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's scale numerical features in range 0 to 1\n",
    "\n",
    "num_cols = ['total_reviews', 'hotel_reviews', 'helpful_votes', 'number_of_rooms']\n",
    "\n",
    "for col in num_cols:\n",
    "    scaler = MinMaxScaler()\n",
    "    train[col] = scaler.fit_transform(train[col].values.reshape(-1, 1))\n",
    "    test[col] = scaler.transform(test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69ea3db509342d650ef9ae0b61c96323280aaf47",
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# target encode the categorical features\n",
    "\n",
    "for col in ['free_wifi', 'club', 'swimming_pool', 'basketball_court', 'exercise_room', 'yoga_classes',\n",
    "               'hotel_stars', 'stayed_in_winter']:\n",
    "    enc_map = train.groupby(by=col)['score'].mean().to_dict()\n",
    "    train[col+'_encoded'] = train[col].map(enc_map)\n",
    "    test[col+'_encoded'] = test[col].map(enc_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Training\n",
    "\n",
    "3 models will be trained on the same data and a weighted average will be taken. The parameters are tuned on CV and LB performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92aa765f3a9d369c26144ed6eae0bc342c454a47",
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Split data in to train and test\n",
    "\n",
    "train_feats, valid_feats, train_labels, valid_labels = train_test_split(train.drop(['score'], axis=1), train['score'],\n",
    "                                                                       test_size=0.10, random_state=2019, stratify=train['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82053188261aeb2aa8387066a2b6fcf2ea211d93",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's frame the problem as a Regression Problem\n",
    "\n",
    "# We will train 3 regression models\n",
    "# The model used are GradientBoostingRegressor, DecisionTreeRegressor and LinearRegression\n",
    "# The data have lot of categorical features, which will help the Tree based DecisionTreeRegressor to find it's way faster\n",
    "# GradientBoostingRegressor on huber loss is very good on small datasets with outliers and its ensembling technique will help\n",
    "# LinearRegression model gave the best result yet.\n",
    "\n",
    "# Let's train all the models and take weighted average and use that to make the final prediction\n",
    "\n",
    "# mapping function to get labels from the predictions made by the regressor\n",
    "def map_prediction(score):\n",
    "    if score < 1.25:\n",
    "        return 1\n",
    "    elif score < 2.25:\n",
    "        return 2\n",
    "    elif score < 3.25:\n",
    "        return 3\n",
    "    elif score < 4.25:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# finding best depth for the DecisionTreeRegressor\n",
    "for d in [2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    gbr = GradientBoostingRegressor(n_estimators=100, loss='huber', random_state=20147)\n",
    "    dtree = DecisionTreeRegressor(max_depth=d, max_leaf_nodes=None, random_state=20147)\n",
    "    lr = LinearRegression(normalize=False)\n",
    "    \n",
    "    gbr.fit(train_feats, train_labels)\n",
    "    dtree.fit(train_feats, train_labels)\n",
    "    lr.fit(train_feats, train_labels)\n",
    "    \n",
    "    valid_preds = dtree.predict(valid_feats)\n",
    "    av = gbr.predict(valid_feats)\n",
    "    lrp = lr.predict(valid_feats)\n",
    "    \n",
    "    valid_preds = 0.3 * av + 0.5* valid_preds + 0.2 * lrp\n",
    "\n",
    "    valid_preds = list(map(map_prediction, valid_preds))\n",
    "    # print(valid_preds)\n",
    "    print(f'Depth : {d} => Score : {accuracy_score(valid_labels, valid_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "99fe5da40638c3275b9a0825742ec5d052e4be36",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Depth 4 seeems best for the DecisionTreeRegressor. \n",
    "\n",
    "Let's train the models on full data and then make prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb292f0675f5fb0e305bd1005fbe78f863054b9e",
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# preparing test set\n",
    "\n",
    "test = test[train.drop(['score'], axis=1).columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "170e1003704853c0bf9f78d56fb3faf32a9b5e7e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# training 3 models on complete train data\n",
    "\n",
    "dtree = DecisionTreeRegressor(max_depth=4, max_leaf_nodes=None, random_state=20147)\n",
    "dtree.fit(train.drop(['score'], axis=1), train['score'])\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, loss='huber', random_state=20147)\n",
    "gbr.fit(train.drop(['score'], axis=1), train['score'])\n",
    "\n",
    "lr = LinearRegression(normalize=False)\n",
    "lr.fit(train.drop(['score'], axis=1), train['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92e4645ef67be5201a4effd81127e8398f82cc20",
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# making prediction\n",
    "\n",
    "test_preds_gbr = gbr.predict(test)\n",
    "test_preds_dtree = dtree.predict(test)\n",
    "test_preds_lr = lr.predict(test)\n",
    "\n",
    "# taking weighted average\n",
    "test_preds = 0.3 * test_preds_gbr + 0.5 * test_preds_dtree + 0.2 * test_preds_lr\n",
    "\n",
    "# mapping predictions to classes\n",
    "test_preds = list(map(map_prediction, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c720de7701d4dfd7156c28a18bb4ebfb00709afd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# loading the sample submission file\n",
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eefac047c391ae22467004b4c0d61a7ae6cc24b2",
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# making submission dataframe\n",
    "\n",
    "sub['score'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ee57d2e0e53daaa8778a5cfad6c1fc7f930a03e",
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# saving to local\n",
    "\n",
    "sub.to_csv('sub_ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2b87b0e86a62c832cf837ff7ff47e170e05d7f5",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ed564c256300c3a969083d66942b781cea52495",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce405d99a1732358ab93c3a49a0a3dd010fac5b1",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
